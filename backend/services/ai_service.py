import os
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
import logging
import json
from pathlib import Path
import re

# Importa√ß√µes do LangChain - vers√µes atualizadas
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.callbacks.manager import get_openai_callback
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# Corrigindo import
from backend.config import get_config

# Garantir que o diret√≥rio de logs existe - CORRIGIDO para usar o diret√≥rio correto
logs_dir = Path(__file__).parent.parent / "logs"  # Usar a pasta logs/ da raiz
logs_dir.mkdir(parents=True, exist_ok=True)

# Configura√ß√£o do logger principal
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(logs_dir / 'api.log'),
        logging.StreamHandler()  # Para mostrar no console
    ]
)
logger = logging.getLogger(__name__)

# Logger espec√≠fico para intera√ß√µes com IA
ai_logger = logging.getLogger('ai_interactions')
ai_logger.setLevel(logging.INFO)

# Remover handlers existentes para evitar duplica√ß√£o
for handler in ai_logger.handlers[:]:
    ai_logger.removeHandler(handler)

# Handler para arquivo de logs de IA separado
ai_log_file = logs_dir / 'ai_interactions.log'
ai_handler = logging.FileHandler(ai_log_file)
ai_formatter = logging.Formatter(
    '%(asctime)s - AI_INTERACTION - %(levelname)s - %(message)s')
ai_handler.setFormatter(ai_formatter)
ai_logger.addHandler(ai_handler)

# Evitar propaga√ß√£o para o logger pai
ai_logger.propagate = False

# Definir n√≠vel de log espec√≠fico para este m√≥dulo
logger.setLevel(logging.INFO)

# Teste inicial do logging
logger.info("=== AI SERVICE INICIADO ===")
ai_logger.info("=== LOGGER DE IA CONFIGURADO E FUNCIONANDO ===")

# Carregar configura√ß√µes
config = get_config()
UPLOAD_FOLDER = config.UPLOAD_FOLDER

# Chave da API do OpenAI - deve vir das vari√°veis de ambiente
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Modelo para sa√≠da estruturada


class QueryResult(BaseModel):
    answer: str = Field(description="Resposta √† consulta do usu√°rio")
    query: str = Field(
        description="Query em pandas para executar nos dados (se aplic√°vel)")
    context: str = Field(
        description="Informa√ß√µes contextuais sobre a resposta")

    # Validator para garantir que answer seja sempre string
    def __init__(self, **data):
        # Converter answer para string se for uma lista
        if 'answer' in data and isinstance(data['answer'], (list, tuple)):
            data['answer'] = "\n".join(map(str, data['answer']))
        super().__init__(**data)


# Parser para sa√≠da estruturada
parser = PydanticOutputParser(pydantic_object=QueryResult)


async def process_query_with_langchain(message: str, file_paths: List[str], history: List[Dict[str, str]] = None) -> Dict[str, str]:
    """
    Processa uma consulta em linguagem natural usando LangChain e modelos de IA
    """
    try:
        # Log inicial da consulta - SIMPLIFICADO
        logger.info(
            f"üîç Nova consulta: '{message[:50]}{'...' if len(message) > 50 else ''}'")
        logger.info(f"üìÅ Arquivos: {len(file_paths)} arquivo(s)")

        # Verificar se temos a chave da API
        if not OPENAI_API_KEY:
            logger.warning(
                "OPENAI_API_KEY n√£o est√° configurada. Usando resposta simulada.")
            return simulate_response(message, file_paths, history)

        # Coletar informa√ß√µes sobre os arquivos CSV
        file_infos = {}
        dataframes = {}

        logger.info(f"üìä Carregando {len(file_paths)} arquivo(s)...")
        for file_path in file_paths:
            filename = Path(file_path).name
            try:
                df = pd.read_csv(file_path)
                dataframes[filename] = df

                logger.info(
                    f"‚úÖ {filename}: {len(df)} linhas, {len(df.columns)} colunas")

                # An√°lise REDUZIDA dos dados - apenas estrutura, n√£o valores
                # APENAS 2 linhas de exemplo, sem estat√≠sticas que revelem resultados
                # Reduzido de 10 para 2 linhas
                sample_data = df.head(2).to_string()

                # INFORMA√á√ïES B√ÅSICAS apenas - sem estat√≠sticas que revelem resultados
                file_info = {
                    "filename": filename,
                    "total_rows": len(df),
                    "columns": list(df.columns),
                    "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
                    "sample_data": sample_data,
                    "structure_info": f"Dataset com {len(df)} registros e {len(df.columns)} colunas. Tipos: {df.dtypes.value_counts().to_dict()}"
                }
                file_infos[filename] = file_info
            except Exception as e:
                logger.error(f"‚ùå Erro ao ler o arquivo {filename}: {str(e)}")
                file_infos[filename] = {
                    "error": str(e)
                }

        # Preparar hist√≥rico para o prompt
        history_text = ""
        if history and len(history) > 0:
            logger.info(
                f"üìö Incluindo hist√≥rico de {len(history)} mensagens anteriores")
            history_text = "Hist√≥rico da conversa:\n"
            for msg in history[-10:]:
                if isinstance(msg, dict):
                    role = "Usu√°rio" if msg.get(
                        "role") == "user" else "Assistente"
                    content = msg.get("content", "")
                else:
                    role = "Usu√°rio" if getattr(
                        msg, 'role', 'user') == "user" else "Assistente"
                    content = getattr(msg, 'content', str(msg))
                history_text += f"{role}: {content}\n"
            history_text += "\n"

        # Prompt melhorado com instru√ß√µes mais espec√≠ficas
        template = """
        Voc√™ √© um assistente especializado em an√°lise de dados CSV. Analise os dados fornecidos e responda √† pergunta do usu√°rio.

        {history}

        DADOS DISPON√çVEIS:
        {file_info}

        PERGUNTA DO USU√ÅRIO: {message}

        INSTRU√á√ïES IMPORTANTES:
        1. Analise os dados reais fornecidos acima
        2. Use EXATAMENTE os nomes das colunas como mostrado nos dados
        3. NUNCA truncar ou abreviar nomes de colunas - use o nome completo
        4. O dataframe principal est√° dispon√≠vel como 'df'
        5. Se h√° m√∫ltiplos arquivos, use df_1, df_2, etc. para acessar espec√≠ficos
        6. Para contar valores, use df['coluna'].value_counts() ou len(df)
        7. Para filtrar, use df[df['coluna'] == 'valor']
        8. Seja preciso com os dados reais mostrados
        9. Se a pergunta envolve m√∫ltiplos arquivos, considere usar df_1, df_2, etc. ou fazer merge/join se apropriado
        10. Se perguntado sobre valores, nas querys formate o resultado como valor monet√°rio (ex: R$ 1.234,56) ou percentual (ex: 12,34%)

        RESPONDA NO FORMATO:
        {format_instructions}
        """

        file_info_str = json.dumps(file_infos, indent=2)

        prompt = PromptTemplate(
            template=template,
            input_variables=["history", "file_info", "message"],
            partial_variables={
                "format_instructions": parser.get_format_instructions()}
        )

        # Configurar modelo
        model = ChatOpenAI(
            model="gpt-4o",
            temperature=0.0,  # Mais determin√≠stico
            api_key=OPENAI_API_KEY
        )

        # Criar e executar a cadeia
        chain = prompt | model | parser

        # NOVO: Log detalhado do que est√° sendo enviado para o modelo
        formatted_prompt = prompt.format(
            history=history_text,
            file_info=file_info_str,
            message=message,
            format_instructions=parser.get_format_instructions()
        )

        # Log do prompt completo que ser√° enviado
        try:
            log_file_path = logs_dir / 'ai_interactions.log'
            with open(log_file_path, 'a', encoding='utf-8') as log_file:
                log_file.write(f"\n{'='*80}\n")
                log_file.write(f"TIMESTAMP: {pd.Timestamp.now()}\n")
                log_file.write(f"PERGUNTA DO USU√ÅRIO: {message}\n")
                log_file.write(
                    f"ARQUIVOS: {[Path(fp).name for fp in file_paths]}\n")
                log_file.write(
                    f"TAMANHO DO PROMPT: {len(formatted_prompt)} caracteres\n")
                log_file.write(f"{'='*80}\n")
                log_file.write(f"PROMPT COMPLETO ENVIADO PARA O MODELO:\n")
                log_file.write(f"{'='*80}\n")
                log_file.write(formatted_prompt)
                log_file.write(f"\n{'='*80}\n\n")
                log_file.flush()  # For√ßar grava√ß√£o
        except Exception as e:
            logger.error(f"Erro ao escrever log manual: {e}")

        logger.info(f"ü§ñ === CONSULTANDO MODELO DE IA ===")
        logger.info(f"üìè Tamanho do prompt: {len(formatted_prompt)} caracteres")

        # Obter resposta
        with get_openai_callback() as cb:
            response = chain.invoke({
                "history": history_text,
                "file_info": file_info_str,
                "message": message
            })
            logger.info(f"üìä Tokens utilizados: {cb.total_tokens}")
            logger.info(f"üí∞ Custo estimado: ${cb.total_cost:.6f}")

            # Log da resposta recebida do modelo - INCLUINDO TODAS AS INFORMA√á√ïES
            try:
                log_file_path = logs_dir / 'ai_interactions.log'
                with open(log_file_path, 'a', encoding='utf-8') as log_file:
                    log_file.write(f"RESPOSTA RECEBIDA DO MODELO:\n")
                    log_file.write(f"{'='*50}\n")
                    log_file.write(f"Answer: {response.answer}\n")
                    log_file.write(f"Query: {response.query}\n")
                    log_file.write(f"Context: {response.context}\n")
                    log_file.write(f"Tokens utilizados: {cb.total_tokens}\n")
                    log_file.write(f"Custo estimado: ${cb.total_cost:.6f}\n")
                    log_file.write(f"{'='*50}\n")
                    log_file.flush()  # For√ßar grava√ß√£o
            except Exception as e:
                logger.error(f"Erro ao escrever log da resposta: {e}")

        logger.info(f"üéØ === RESPOSTA DO MODELO ===")
        logger.info(f"Resposta: {response.answer}")
        logger.info(f"Query gerada: {response.query}")
        logger.info(f"Contexto: {response.context}")

        # Executar a query se fornecida
        if response.query and response.query.strip():
            logger.info(
                f"‚öôÔ∏è Executando query: {response.query[:100]}{'...' if len(response.query) > 100 else ''}")
            query_executed_successfully = False
            try:
                # Execu√ß√£o mais segura da query
                result = execute_pandas_query(response.query, dataframes)

                if result is not None:
                    logger.info(
                        f"‚úÖ Query executada com sucesso! Tipo: {type(result).__name__}")
                    query_executed_successfully = True

                    # Log do resultado da query executada
                    try:
                        log_file_path = logs_dir / 'ai_interactions.log'
                        with open(log_file_path, 'a', encoding='utf-8') as log_file:
                            log_file.write(
                                f"RESULTADO DA EXECU√á√ÉO DA QUERY:\n")
                            log_file.write(f"{'='*50}\n")
                            log_file.write(
                                f"Query executada: {response.query}\n")
                            log_file.write(
                                f"Tipo do resultado: {type(result)}\n")

                            if isinstance(result, (int, float)):
                                log_file.write(f"Valor num√©rico: {result}\n")
                            elif isinstance(result, pd.DataFrame):
                                log_file.write(
                                    f"DataFrame: {len(result)} linhas, {len(result.columns)} colunas\n")
                                if len(result) <= 10:
                                    log_file.write(
                                        f"Resultado completo:\n{result.to_string()}\n")
                                else:
                                    log_file.write(
                                        f"Primeiras 5 linhas:\n{result.head(5).to_string()}\n")
                            elif isinstance(result, pd.Series):
                                log_file.write(
                                    f"Series: {len(result)} valores\n")
                                if len(result) <= 20:
                                    log_file.write(
                                        f"Resultado:\n{result.to_string()}\n")
                                else:
                                    log_file.write(
                                        f"Primeiros 10 valores:\n{result.head(10).to_string()}\n")
                            else:
                                log_file.write(
                                    f"Resultado: {str(result)[:500]}\n")

                            log_file.write(f"Status: Execu√ß√£o bem-sucedida\n")
                            log_file.write(f"{'='*50}\n\n")
                            log_file.flush()
                    except Exception as e:
                        logger.error(f"Erro ao escrever log do resultado: {e}")

                    # Atualizar a resposta com o resultado real
                    if isinstance(result, (int, float)):
                        # Tratamento especial para c√°lculos de tempo/atraso
                        if "atraso" in response.query.lower() or "tempo" in response.query.lower():
                            if result < 0:
                                response.answer = f"Em m√©dia, as cirurgias come√ßam {abs(result):.1f} minutos ANTES do hor√°rio agendado (n√£o h√° atraso, mas sim antecipa√ß√£o)."
                            elif result == 0:
                                response.answer = f"Em m√©dia, as cirurgias come√ßam exatamente no hor√°rio agendado (atraso m√©dio de 0 minutos)."
                            else:
                                response.answer = f"Em m√©dia, as cirurgias atrasam {result:.1f} minutos para come√ßar."
                        else:
                            response.answer = f"Resultado: {result}"
                        logger.info(f"üìà Resultado: {result}")
                    elif isinstance(result, pd.DataFrame):
                        if len(result) <= 20:  # Se for pequeno, mostrar tudo
                            response.answer = f"Resultado:\n{result.to_string()}"
                        else:
                            response.answer = f"Resultado (primeiras 10 linhas):\n{result.head(10).to_string()}\n\nTotal de {len(result)} registros encontrados."
                        logger.info(
                            f"üìä DataFrame: {len(result)} linhas")
                    elif isinstance(result, pd.Series):
                        response.answer = f"Resultado:\n{result.to_string()}"
                        logger.info(
                            f"üìã Series: {len(result)} valores")
                    elif isinstance(result, tuple):
                        # Formata√ß√£o especial para tuplas (como nome do m√©dico + quantidade)
                        if len(result) == 2:
                            response.answer = f"Resultado: {result[0]} com {result[1]} ocorr√™ncias"
                        else:
                            response.answer = f"Resultado: {result}"
                        logger.info(f"üìÑ Tupla: {result}")
                    else:
                        response.answer = f"Resultado: {result}"
                        logger.info(f"üìÑ Resultado: {type(result).__name__}")

                    response.context += f"\n\nQuery executada com sucesso: {response.query}"

            except Exception as e:
                logger.error(
                    f"‚ùå Erro na execu√ß√£o: {str(e)[:100]}{'...' if len(str(e)) > 100 else ''}")
                query_executed_successfully = False

                # IMPORTANTE: Se a query falhou, n√£o retornar dados inventados
                # Retornar uma mensagem de erro clara
                error_message = (
                    f"‚ùå N√£o consegui processar sua consulta devido a um erro t√©cnico:\n\n"
                    f"**Erro:** {str(e)}\n\n"
                    f"**Sugest√µes:**\n"
                    f"‚Ä¢ Tente reformular sua pergunta de forma mais espec√≠fica\n"
                    f"‚Ä¢ Verifique se os nomes das colunas est√£o corretos\n"
                    f"‚Ä¢ Para c√°lculos de tempo, seja mais expl√≠cito sobre o formato desejado\n\n"
                    f"**Exemplo de perguntas que funcionam bem:**\n"
                    f"‚Ä¢ 'Quantas cirurgias foram realizadas?'\n"
                    f"‚Ä¢ 'Qual m√©dico fez mais cirurgias?'\n"
                    f"‚Ä¢ 'Liste as cirurgias de ortopedia'"
                )

                return {
                    "answer": error_message,
                    "query": response.query,
                    "context": f"Erro na execu√ß√£o da query: {str(e)}"
                }
        else:
            logger.info(f"‚ÑπÔ∏è Nenhuma query gerada")

        logger.info(f"üèÅ Consulta finalizada - resposta gerada com sucesso")
        logger.info(f"Resposta final: {response.answer[:200]}...")

        return {
            "answer": str(response.answer),
            "query": response.query,
            "context": response.context
        }

    except Exception as e:
        logger.error(f"üí• Erro geral ao processar consulta: {str(e)}")
        return simulate_response(message, file_paths, history)


def execute_pandas_query(query: str, dataframes: Dict[str, pd.DataFrame]):
    """
    Executa uma query pandas de forma mais segura
    """
    try:
        # Limpar o c√≥digo
        code = clean_code_block(query)

        # Log simplificado para console - sem poluir com detalhes
        logger.info(f"‚öôÔ∏è Executando query pandas...")

        # Preparar ambiente seguro com TODAS as fun√ß√µes built-in necess√°rias
        safe_builtins = {
            'len': len,
            'str': str,
            'int': int,
            'float': float,
            'list': list,
            'dict': dict,
            'tuple': tuple,
            'set': set,
            'sum': sum,
            'min': min,
            'max': max,
            'abs': abs,
            'round': round,
            'sorted': sorted,
            'enumerate': enumerate,
            'range': range,
            'zip': zip,
        }

        safe_globals = {
            'pd': pd,
            'np': np,
            '__builtins__': safe_builtins
        }

        # Adicionar dataframes
        local_vars = {}
        for i, (filename, df) in enumerate(dataframes.items()):
            # Fazer uma c√≥pia para n√£o modificar o original
            local_vars[f'df_{i+1}'] = df.copy()

        # SEMPRE disponibilizar 'df' como o dataframe principal
        # Se h√° apenas um dataframe, usar esse
        # Se h√° m√∫ltiplos, usar o primeiro como padr√£o
        main_df = list(dataframes.values())[0].copy()
        local_vars['df'] = main_df

        # Log simplificado dos dataframes dispon√≠veis
        if len(dataframes) == 1:
            logger.info(
                f"üìä Dataframe √∫nico: {len(main_df)} linhas, {len(main_df.columns)} colunas")
        else:
            logger.info(
                f"üìä M√∫ltiplos dataframes: {len(dataframes)} arquivos carregados")

        exec_globals = {**safe_globals, **local_vars}

        # Verificar se √© uma query de m√∫ltiplas linhas ou com assignments
        has_assignment = '=' in code and not any([
            code.strip().startswith('('),
            '==' in code,  # Compara√ß√µes n√£o s√£o assignments
            '!=' in code,  # Compara√ß√µes n√£o s√£o assignments
            '>=' in code,  # Compara√ß√µes n√£o s√£o assignments
            '<=' in code,  # Compara√ß√µes n√£o s√£o assignments
        ])
        has_multiple_statements = ';' in code or '\n' in code.strip()

        if has_assignment or has_multiple_statements:
            # Para queries complexas, usar exec
            # Executar e capturar resultado
            local_namespace = exec_globals.copy()
            exec(code, local_namespace)

            # Tentar encontrar vari√°veis de resultado comuns
            result_vars = ['result', 'media_atraso',
                           'atraso_medio', 'resposta', 'output']
            result = None

            for var in result_vars:
                if var in local_namespace:
                    result = local_namespace[var]
                    break

            # Se n√£o encontrou resultado em vari√°veis, tentar o √∫ltimo valor definido
            if result is None:
                # Procurar por vari√°veis que n√£o estavam no namespace original
                new_vars = {k: v for k, v in local_namespace.items()
                            if k not in exec_globals and not k.startswith('__')}
                if new_vars:
                    # Pegar a √∫ltima vari√°vel criada
                    last_var = list(new_vars.keys())[-1]
                    result = new_vars[last_var]

        else:
            # Para queries simples, usar eval
            result = eval(code, exec_globals)

        # Log simplificado do resultado - apenas tipo e resumo
        logger.info(f"‚úÖ Query executada: {type(result).__name__}")

        return result

    except Exception as e:
        logger.error(
            f"‚ùå Erro na execu√ß√£o da query: {str(e)[:100]}{'...' if len(str(e)) > 100 else ''}")
        raise e


def clean_code_block(code: str) -> str:
    """Remove marcadores de blocos de c√≥digo Markdown, se presentes."""
    code = code.strip()
    if code.startswith("```python"):
        code = code[9:]
    elif code.startswith("```"):
        code = code[3:]
    if code.endswith("```"):
        code = code[:-3]
    return code.strip()


def simulate_response(message: str, file_paths: List[str], history: List[Dict[str, str]] = None) -> Dict[str, str]:
    """
    Gera uma resposta simulada quando o modelo de IA n√£o est√° dispon√≠vel

    Args:
        message: A mensagem/pergunta do usu√°rio
        file_paths: Lista de caminhos para os arquivos CSV
        history: Lista de mensagens anteriores da conversa

    Returns:
        Dict[str, str]: Resposta simulada
    """
    # Coletar informa√ß√µes b√°sicas sobre os arquivos
    file_names = [Path(path).name for path in file_paths]
    file_info = []

    for path in file_paths:
        try:
            filename = Path(path).name
            df = pd.read_csv(path)
            file_info.append({
                "filename": filename,
                "rows": len(df),
                "columns": list(df.columns)
            })
        except Exception as e:
            file_info.append({
                "filename": Path(path).name,
                "error": str(e)
            })

    query_lower = message.lower()

    # Verificar contexto do hist√≥rico para respostas mais inteligentes
    context_from_history = ""
    if history and len(history) > 0:
        last_messages = [msg.get("content", "") for msg in history[-3:]]
        context_from_history = f" (Com base na conversa anterior sobre: {', '.join(last_messages[-1:])})"

    # Gerar uma resposta com base em palavras-chave
    if any(word in query_lower for word in ["m√©dia", "media", "average", "mean"]):
        return {
            "answer": f"Para calcular a m√©dia, eu precisaria analisar os dados num√©ricos nos arquivos: {', '.join(file_names)}.{context_from_history}",
            "query": "df.select_dtypes(include=['number']).mean()",
            "context": "Esta √© uma resposta simulada. Quando a API OpenAI estiver configurada, fornecerei an√°lises reais dos dados."
        }
    elif any(word in query_lower for word in ["contar", "count", "quantos", "quanto"]):
        return {
            "answer": f"Para contar ocorr√™ncias, eu analisaria a frequ√™ncia de valores nos arquivos: {', '.join(file_names)}.{context_from_history}",
            "query": "df.value_counts()",
            "context": "Esta √© uma resposta simulada. Quando a API OpenAI estiver configurada, fornecerei an√°lises reais dos dados."
        }
    elif any(word in query_lower for word in ["mostrar", "show", "exibir", "listar", "list"]):
        return {
            "answer": f"Aqui estaria uma pr√©via dos dados dos arquivos: {', '.join(file_names)}.{context_from_history}",
            "query": "df.head(5)",
            "context": "Esta √© uma resposta simulada. Quando a API OpenAI estiver configurada, fornecerei an√°lises reais dos dados."
        }
    else:
        # Resumo dos arquivos dispon√≠veis
        files_info = "\n".join([f"- {info['filename']}: {info.get('rows', 'N/A')} linhas, {len(info.get('columns', []))} colunas"
                                for info in file_info])

        return {
            "answer": f"Posso analisar os seguintes arquivos CSV:\n{files_info}{context_from_history}",
            "query": "",
            "context": "Esta √© uma resposta simulada. Por favor, configure a vari√°vel de ambiente OPENAI_API_KEY para habilitar respostas reais do modelo de IA."
        }
