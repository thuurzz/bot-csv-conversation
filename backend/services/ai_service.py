import os
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
import logging
import json
from pathlib import Path
import re

# Importa√ß√µes do LangChain - vers√µes atualizadas
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.callbacks.manager import get_openai_callback
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# Corrigindo import
from backend.config import get_config

# Configura√ß√£o do logger com mais detalhes
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('backend/logs/api.log'),
        logging.StreamHandler()  # Para mostrar no console
    ]
)
logger = logging.getLogger(__name__)

# Definir n√≠vel de log espec√≠fico para este m√≥dulo
logger.setLevel(logging.INFO)

# Carregar configura√ß√µes
config = get_config()
UPLOAD_FOLDER = config.UPLOAD_FOLDER

# Chave da API do OpenAI - deve vir das vari√°veis de ambiente
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Modelo para sa√≠da estruturada


class QueryResult(BaseModel):
    answer: str = Field(description="Resposta √† consulta do usu√°rio")
    query: str = Field(
        description="Query em pandas para executar nos dados (se aplic√°vel)")
    context: str = Field(
        description="Informa√ß√µes contextuais sobre a resposta")

    # Validator para garantir que answer seja sempre string
    def __init__(self, **data):
        # Converter answer para string se for uma lista
        if 'answer' in data and isinstance(data['answer'], (list, tuple)):
            data['answer'] = "\n".join(map(str, data['answer']))
        super().__init__(**data)


# Parser para sa√≠da estruturada
parser = PydanticOutputParser(pydantic_object=QueryResult)


async def process_query_with_langchain(message: str, file_paths: List[str], history: List[Dict[str, str]] = None) -> Dict[str, str]:
    """
    Processa uma consulta em linguagem natural usando LangChain e modelos de IA
    """
    try:
        # Log inicial da consulta
        logger.info(f"üîç === NOVA CONSULTA RECEBIDA ===")
        logger.info(f"Pergunta do usu√°rio: {message}")
        logger.info(
            f"Arquivos selecionados: {[Path(fp).name for fp in file_paths]}")

        # Verificar se temos a chave da API
        if not OPENAI_API_KEY:
            logger.warning(
                "OPENAI_API_KEY n√£o est√° configurada. Usando resposta simulada.")
            return simulate_response(message, file_paths, history)

        # Coletar informa√ß√µes sobre os arquivos CSV
        file_infos = []
        dataframes = {}

        logger.info(f"üìä === CARREGANDO DADOS DOS ARQUIVOS ===")
        for file_path in file_paths:
            filename = Path(file_path).name
            try:
                df = pd.read_csv(file_path)
                dataframes[filename] = df

                logger.info(
                    f"‚úÖ Arquivo '{filename}' carregado: {len(df)} linhas, {len(df.columns)} colunas")

                # An√°lise mais detalhada dos dados para melhor contexto
                sample_data = df.head(10).to_string()  # Mais dados de exemplo
                unique_counts = {}
                for col in df.columns:
                    if df[col].dtype == 'object':
                        # Primeiros 10 valores √∫nicos
                        unique_vals = df[col].dropna().unique()[:10]
                        unique_counts[col] = list(unique_vals)

                file_info = {
                    "filename": filename,
                    "total_rows": len(df),
                    "columns": list(df.columns),
                    "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
                    "sample_data": sample_data,
                    "unique_values": unique_counts,
                    "summary_stats": df.describe(include='all').to_string() if len(df) > 0 else "DataFrame vazio"
                }
                file_infos.append(file_info)
            except Exception as e:
                logger.error(f"‚ùå Erro ao ler o arquivo {filename}: {str(e)}")
                file_infos.append({
                    "filename": filename,
                    "error": str(e)
                })

        # Preparar hist√≥rico para o prompt
        history_text = ""
        if history and len(history) > 0:
            logger.info(
                f"üìö Incluindo hist√≥rico de {len(history)} mensagens anteriores")
            history_text = "Hist√≥rico da conversa:\n"
            for msg in history[-10:]:
                if isinstance(msg, dict):
                    role = "Usu√°rio" if msg.get(
                        "role") == "user" else "Assistente"
                    content = msg.get("content", "")
                else:
                    role = "Usu√°rio" if getattr(
                        msg, 'role', 'user') == "user" else "Assistente"
                    content = getattr(msg, 'content', str(msg))
                history_text += f"{role}: {content}\n"
            history_text += "\n"

        # Prompt melhorado com instru√ß√µes mais espec√≠ficas
        template = """
        Voc√™ √© um assistente especializado em an√°lise de dados CSV. Analise os dados fornecidos e responda √† pergunta do usu√°rio.

        {history}

        DADOS DISPON√çVEIS:
        {file_info}

        PERGUNTA DO USU√ÅRIO: {message}

        INSTRU√á√ïES IMPORTANTES:
        1. Analise os dados reais fornecidos acima
        2. Se precisar gerar c√≥digo pandas, use apenas opera√ß√µes simples e seguras
        3. O dataframe principal est√° dispon√≠vel como 'df'
        4. Para contar valores, use df['coluna'].value_counts() ou len(df)
        5. Para filtrar, use df[df['coluna'] == 'valor']
        6. Seja preciso com os dados reais mostrados

        RESPONDA NO FORMATO:
        {format_instructions}
        """

        file_info_str = json.dumps(file_infos, indent=2)

        prompt = PromptTemplate(
            template=template,
            input_variables=["history", "file_info", "message"],
            partial_variables={
                "format_instructions": parser.get_format_instructions()}
        )

        # Configurar modelo
        model = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.0,  # Mais determin√≠stico
            api_key=OPENAI_API_KEY
        )

        # Criar e executar a cadeia
        chain = prompt | model | parser

        logger.info(f"ü§ñ === CONSULTANDO MODELO DE IA ===")
        # Obter resposta
        with get_openai_callback() as cb:
            response = chain.invoke({
                "history": history_text,
                "file_info": file_info_str,
                "message": message
            })
            logger.info(f"üìä Tokens utilizados: {cb.total_tokens}")
            logger.info(f"üí∞ Custo estimado: ${cb.total_cost:.6f}")

        logger.info(f"üéØ === RESPOSTA DO MODELO ===")
        logger.info(f"Resposta: {response.answer}")
        logger.info(f"Query gerada: {response.query}")
        logger.info(f"Contexto: {response.context}")

        # Executar a query se fornecida
        if response.query and response.query.strip():
            logger.info(f"‚öôÔ∏è === INICIANDO EXECU√á√ÉO DA QUERY ===")
            try:
                # Execu√ß√£o mais segura da query
                result = execute_pandas_query(response.query, dataframes)

                if result is not None:
                    logger.info(f"‚úÖ Query executada com sucesso!")

                    # Atualizar a resposta com o resultado real
                    if isinstance(result, (int, float)):
                        response.answer = f"Resultado: {result}"
                        logger.info(f"üìà Resultado num√©rico: {result}")
                    elif isinstance(result, pd.DataFrame):
                        if len(result) <= 20:  # Se for pequeno, mostrar tudo
                            response.answer = f"Resultado:\n{result.to_string()}"
                        else:
                            response.answer = f"Resultado (primeiras 10 linhas):\n{result.head(10).to_string()}\n\nTotal de {len(result)} registros encontrados."
                        logger.info(
                            f"üìä DataFrame resultado com {len(result)} linhas")
                    elif isinstance(result, pd.Series):
                        response.answer = f"Resultado:\n{result.to_string()}"
                        logger.info(
                            f"üìã Series resultado com {len(result)} valores")
                    else:
                        response.answer = f"Resultado: {result}"
                        logger.info(
                            f"üìÑ Outro tipo de resultado: {type(result)}")

                    response.context += f"\n\nQuery executada: {response.query}"

            except Exception as e:
                logger.error(f"‚ùå Erro ao executar query: {str(e)}")
                response.context += f"\n\nErro ao executar query: {str(e)}"
        else:
            logger.info(f"‚ÑπÔ∏è Nenhuma query foi gerada ou necess√°ria")

        logger.info(f"üèÅ === CONSULTA FINALIZADA ===")
        logger.info(f"Resposta final: {response.answer[:200]}...")

        return {
            "answer": str(response.answer),
            "query": response.query,
            "context": response.context
        }

    except Exception as e:
        logger.error(f"üí• Erro geral ao processar consulta: {str(e)}")
        return simulate_response(message, file_paths, history)


def execute_pandas_query(query: str, dataframes: Dict[str, pd.DataFrame]):
    """
    Executa uma query pandas de forma mais segura
    """
    try:
        # Limpar o c√≥digo
        code = clean_code_block(query)

        # Log da query que ser√° executada
        logger.info(f"=== EXECUTANDO QUERY ===")
        logger.info(f"Query original: {query}")
        logger.info(f"Query limpa: {code}")

        # Log dos dataframes dispon√≠veis
        for filename, df in dataframes.items():
            logger.info(
                f"Dataframe '{filename}': {len(df)} linhas, {len(df.columns)} colunas")
            logger.info(f"Colunas: {list(df.columns)}")
            logger.info(f"Primeiras 3 linhas:\n{df.head(3).to_string()}")

        # Preparar ambiente seguro com TODAS as fun√ß√µes built-in necess√°rias
        safe_builtins = {
            'len': len,
            'str': str,
            'int': int,
            'float': float,
            'list': list,
            'dict': dict,
            'tuple': tuple,
            'set': set,
            'sum': sum,
            'min': min,
            'max': max,
            'abs': abs,
            'round': round,
            'sorted': sorted,
            'enumerate': enumerate,
            'range': range,
            'zip': zip,
        }

        safe_globals = {
            'pd': pd,
            'np': np,
            '__builtins__': safe_builtins
        }

        # Adicionar dataframes
        local_vars = {}
        for i, (filename, df) in enumerate(dataframes.items()):
            local_vars[f'df_{i+1}'] = df

        # Se h√° apenas um dataframe, disponibilizar como 'df'
        if len(dataframes) == 1:
            local_vars['df'] = list(dataframes.values())[0]
            logger.info(
                f"Dataframe principal 'df' configurado com {len(local_vars['df'])} linhas")

        # Executar sempre com eval para queries simples
        exec_globals = {**safe_globals, **local_vars}

        logger.info(f"Executando com eval: {code}")
        result = eval(code, exec_globals)

        # Log do resultado
        logger.info(f"=== RESULTADO DA QUERY ===")
        logger.info(f"Tipo do resultado: {type(result)}")

        if isinstance(result, (int, float, str)):
            logger.info(f"Valor: {result}")
        elif isinstance(result, pd.DataFrame):
            logger.info(
                f"DataFrame resultado: {len(result)} linhas, {len(result.columns)} colunas")
            logger.info(f"Resultado completo:\n{result.to_string()}")
        elif isinstance(result, pd.Series):
            logger.info(f"Series resultado: {len(result)} valores")
            logger.info(f"Resultado completo:\n{result.to_string()}")
        else:
            # Limitar log se for muito grande
            logger.info(f"Resultado: {str(result)[:500]}...")

        logger.info(f"=== FIM DA EXECU√á√ÉO ===")
        return result

    except Exception as e:
        logger.error(f"=== ERRO NA EXECU√á√ÉO DA QUERY ===")
        logger.error(f"Query: {code}")
        logger.error(f"Erro: {str(e)}")
        logger.error(f"=== FIM DO ERRO ===")
        raise e


def clean_code_block(code: str) -> str:
    """Remove marcadores de blocos de c√≥digo Markdown, se presentes."""
    code = code.strip()
    if code.startswith("```python"):
        code = code[9:]
    elif code.startswith("```"):
        code = code[3:]
    if code.endswith("```"):
        code = code[:-3]
    return code.strip()


def simulate_response(message: str, file_paths: List[str], history: List[Dict[str, str]] = None) -> Dict[str, str]:
    """
    Gera uma resposta simulada quando o modelo de IA n√£o est√° dispon√≠vel

    Args:
        message: A mensagem/pergunta do usu√°rio
        file_paths: Lista de caminhos para os arquivos CSV
        history: Lista de mensagens anteriores da conversa

    Returns:
        Dict[str, str]: Resposta simulada
    """
    # Coletar informa√ß√µes b√°sicas sobre os arquivos
    file_names = [Path(path).name for path in file_paths]
    file_info = []

    for path in file_paths:
        try:
            filename = Path(path).name
            df = pd.read_csv(path)
            file_info.append({
                "filename": filename,
                "rows": len(df),
                "columns": list(df.columns)
            })
        except Exception as e:
            file_info.append({
                "filename": Path(path).name,
                "error": str(e)
            })

    query_lower = message.lower()

    # Verificar contexto do hist√≥rico para respostas mais inteligentes
    context_from_history = ""
    if history and len(history) > 0:
        last_messages = [msg.get("content", "") for msg in history[-3:]]
        context_from_history = f" (Com base na conversa anterior sobre: {', '.join(last_messages[-1:])})"

    # Gerar uma resposta com base em palavras-chave
    if any(word in query_lower for word in ["m√©dia", "media", "average", "mean"]):
        return {
            "answer": f"Para calcular a m√©dia, eu precisaria analisar os dados num√©ricos nos arquivos: {', '.join(file_names)}.{context_from_history}",
            "query": "df.select_dtypes(include=['number']).mean()",
            "context": "Esta √© uma resposta simulada. Quando a API OpenAI estiver configurada, fornecerei an√°lises reais dos dados."
        }
    elif any(word in query_lower for word in ["contar", "count", "quantos", "quanto"]):
        return {
            "answer": f"Para contar ocorr√™ncias, eu analisaria a frequ√™ncia de valores nos arquivos: {', '.join(file_names)}.{context_from_history}",
            "query": "df.value_counts()",
            "context": "Esta √© uma resposta simulada. Quando a API OpenAI estiver configurada, fornecerei an√°lises reais dos dados."
        }
    elif any(word in query_lower for word in ["mostrar", "show", "exibir", "listar", "list"]):
        return {
            "answer": f"Aqui estaria uma pr√©via dos dados dos arquivos: {', '.join(file_names)}.{context_from_history}",
            "query": "df.head(5)",
            "context": "Esta √© uma resposta simulada. Quando a API OpenAI estiver configurada, fornecerei an√°lises reais dos dados."
        }
    else:
        # Resumo dos arquivos dispon√≠veis
        files_info = "\n".join([f"- {info['filename']}: {info.get('rows', 'N/A')} linhas, {len(info.get('columns', []))} colunas"
                                for info in file_info])

        return {
            "answer": f"Posso analisar os seguintes arquivos CSV:\n{files_info}{context_from_history}",
            "query": "",
            "context": "Esta √© uma resposta simulada. Por favor, configure a vari√°vel de ambiente OPENAI_API_KEY para habilitar respostas reais do modelo de IA."
        }
